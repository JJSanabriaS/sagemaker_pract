{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f87bc0c-367f-4716-9f0e-2587b79148ce",
   "metadata": {},
   "source": [
    "# Accelerating LLM Deployments with SageMaker Fast Model Loader\n",
    "\n",
    "Amazon SageMaker Fast Model Loader represents a significant advancement in deploying Large Language Models (LLMs) for inference. As LLMs continue to grow in size and complexity, with some models requiring hundreds of gigabytes of memory, the traditional model loading process has become a major bottleneck in deployment and scaling.\n",
    "\n",
    "This notebook demonstrates how to leverage Fast Model Loader to dramatically improve model loading times. The feature works by streaming model weights directly from Amazon S3 to GPU accelerators, bypassing the typical sequential loading steps that contribute to deployment latency. In internal testing, this approach has shown to load large models up to 15 times faster compared to traditional methods.\n",
    "\n",
    "We'll walk through deploying the Llama 3.1 70B model using Fast Model Loader, showcasing how to:\n",
    "- Optimize the model for streaming using ModelBuilder\n",
    "- Configure tensor parallelism for distributed inference\n",
    "- Deploy the optimized model to a SageMaker endpoint\n",
    "- Test the deployment with inference requests\n",
    "\n",
    "Fast Model Loader introduces two key techniques that work together:\n",
    "1. Weight Streaming - Directly streams model weights from S3 to GPU memory\n",
    "2. Model Sharding for Streaming - Pre-shards the model in uniform chunks for optimal loading\n",
    "\n",
    "## Prerequisites\n",
    "- SageMaker execution role with appropriate permissions\n",
    "- Access to a GPU instance (ml.p4d.24xlarge recommended for this example)\n",
    "- SageMaker Python SDK\n",
    "\n",
    "\n",
    "## Setup Environment\n",
    "First, we'll set up our SageMaker session and define basic variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3723cf-16c2-4f96-a977-2f3c2691dd7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir sagemaker==2.235.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb5d9c-66e7-4d0a-a816-002bc14fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "role=get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sess = Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f4cda-692d-42b4-8501-8f3c561438c0",
   "metadata": {},
   "source": [
    "## Create Model Builder\n",
    "We'll use the ModelBuilder class to prepare and package the model inference components. In this example, we're using the Llama 3.1 70B model from SageMaker JumpStart.\n",
    "\n",
    "Key configurations:\n",
    "- Model: meta-textgeneration-llama-3-1-70b\n",
    "- Schema Builder: Defines input/output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9066d-c9f6-45c7-9751-f33b8c776d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "import logging\n",
    "model_builder = ModelBuilder(\n",
    "    model=\"meta-textgeneration-llama-3-1-70b\",\n",
    "    role_arn=role,\n",
    "    sagemaker_session=sess,\n",
    "    schema_builder=SchemaBuilder(sample_input=\"Test\", sample_output=\"Test\"),\n",
    "    #env_vars={\n",
    "    #   \"OPTION_TENSOR_PARALLEL_DEGREE\": \"8\",\n",
    "    #},\n",
    "    log_level=logging.WARN\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{bucket}/sharding\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37f9e9",
   "metadata": {},
   "source": [
    "Note that, if you have already run the model optimization job before and the model shards are available on s3. You can uncomment below code to reuse the existing model shards and skip the section `Optimize Model for Fast Loading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_builder = ModelBuilder(\n",
    "#             model=\"meta-textgeneration-llama-3-1-70b\",\n",
    "#             model_metadata={\n",
    "#                 \"CUSTOM_MODEL_PATH\": output_path,\n",
    "#             },\n",
    "#             schema_builder=SchemaBuilder(sample_input=\"Test\", sample_output=\"Test\"),\n",
    "#             role_arn=role,\n",
    "#             instance_type=\"ml.g5.48xlarge\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0769e-68f0-4f9a-afb0-47874c456bf3",
   "metadata": {},
   "source": [
    "## Optimize Model for Fast Loading\n",
    "Now we'll optimize the model using Fast Model Loader. This process:\n",
    "1. Prepares model shards for deployment\n",
    "2. Enables direct streaming from S3 to GPU\n",
    "3. Pre-configures tensor parallelism settings\n",
    "\n",
    "Note: The optimization process may take a while to complete. The optimized model will be stored in the specified S3 output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0286e-1f88-44d4-877b-a1bda0bd2e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_builder.optimize(\n",
    "    instance_type=\"ml.p4d.24xlarge\", \n",
    "    accept_eula=True, \n",
    "    output_path=output_path, \n",
    "    sharding_config={\n",
    "            \"OverrideEnvironment\": {\n",
    "                \"OPTION_TENSOR_PARALLEL_DEGREE\": \"8\"\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc389b-6208-4930-be07-f1c8eb063b91",
   "metadata": {},
   "source": [
    "## Build and Deploy Model\n",
    "After optimization, we'll build the final model artifacts and deploy them to a SageMaker endpoint. \n",
    "\n",
    "Key configurations:\n",
    "- Instance Type: ml.p4d.24xlarge\n",
    "- Memory Request: 204800 MB\n",
    "- Number of Accelerators: 8 (for tensor parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58566f77-a18f-4606-b8fa-67f3040eac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec6a6a-2547-4df2-aed5-636837107426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should use the sharded model\n",
    "if not final_model._is_sharded_model:\n",
    "    final_model._is_sharded_model = True\n",
    "final_model._is_sharded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnableNetworkIsolation cannot be set to True since SageMaker Fast Model Loading of model requires network access.\n",
    "if final_model._enable_network_isolation:\n",
    "    final_model._enable_network_isolation = False\n",
    "final_model._enable_network_isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "resources_required = ResourceRequirements(\n",
    "    requests={\n",
    "        \"memory\" : 204800,\n",
    "        \"num_accelerators\": 8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bcf66-5381-4bb2-90ab-3d2efdc2e5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.deploy(\n",
    "    instance_type=\"ml.p4d.24xlarge\", \n",
    "    accept_eula=True, \n",
    "    # endpoint_logging=False, \n",
    "    resources=resources_required,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f8794-0c6c-4cbd-8795-5a497eea3025",
   "metadata": {},
   "source": [
    "## Test the Endpoint\n",
    "Finally, we'll test the deployed endpoint with a simple inference request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0233a-8e9f-4084-9b35-d4df69937b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default \n",
    "\n",
    "endpoint_name = final_model.endpoint_name \n",
    "predictor = retrieve_default(endpoint_name, sagemaker_session=sess) \n",
    "\n",
    "payload = { \"inputs\": \"I believe the meaning of life is\", \n",
    "            \"parameters\": { \n",
    "                \"max_new_tokens\": 64, \n",
    "                \"top_p\": 0.9, \n",
    "                \"temperature\": 0.6 \n",
    "            } \n",
    "        }\n",
    "response = predictor.predict(payload) \n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f34636-3fd2-472b-9bd5-cb8145c4e1ba",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c5704-f5c4-424e-a392-bf0e1f1d1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_predictor()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c37a59-9d76-4d7d-9ccb-5fab6879e443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
